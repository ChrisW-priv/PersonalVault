---
title: LLM performance benchmark
description: Set of links to websites that enable easy compare of the performance of the models
date: 2025-03-28
tags:
  - LLM
categories:
  - zettelkasten
draft: true
---

TLDR: My latest recommendation:

on a laptop, it is best to use deepseek-r1:1.5b for thinking and hermes3:3b for execution. but you need good laptop, hard to make it work with aider, I tried.

on a GPU powered machine: the same but more params (bigger models).

## Coding performance

By this, I mean problem solving ability

- [link](https://www.prollm.ai/leaderboard/stack-eval?type=conceptual,debugging,implementation,optimization&level=advanced,beginner,intermediate&tag=python)
- [link](https://huggingface.co/spaces/mike-ravkine/can-ai-code-results)

## Instruction following

I just have: [link](https://www.reddit.com/r/LocalLLaMA/comments/1hd2eh2/best_model_for_instruction_following_to_date/)
