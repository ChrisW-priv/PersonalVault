---
title: LLM performance benchmark
description: Set of links to websites that enable easy compare of the performance of the models
date: 2025-03-28
tags:
  - LLM
categories:
  - zettelkasten
draft: true
---

TLDR: My latest recomendation:
on a laptop, it is best to use deepseek-r1:1.5b for thinking and hermes3:3b for execution.
on a GPU powered machine: the same but more params (bigger models)

## Coding performance

By this, I mean problem solving ability

- [link](https://www.prollm.ai/leaderboard/stack-eval?type=conceptual,debugging,implementation,optimization&level=advanced,beginner,intermediate&tag=python)

## Instruction following

I just have: [link](https://www.reddit.com/r/LocalLLaMA/comments/1hd2eh2/best_model_for_instruction_following_to_date/)
