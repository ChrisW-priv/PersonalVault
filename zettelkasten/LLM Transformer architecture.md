# LLM Transformer architecture 


## What are the parts of a transformer?
Embeddings    (encode the token from text into a vector of numbers)
loop n
    Attention (how the token relates to all the other tokens)
    MLPs      (updating the vector values based on a series of "questions")
Unembedding   (decode the token from a vector into text)

